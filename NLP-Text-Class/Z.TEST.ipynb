{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 셋 로딩 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(s3_amz_review_apparel_negative_path, nrows= 14000, \n",
    "                 encoding='utf-8',engine='python')\n",
    "# df = pd.read_csv(s3_amz_review_apparel_negative_path,error_bad_lines=False,\n",
    "#                  encoding='utf-8',engine='python')\n",
    "\n",
    "# df = pd.read_csv(s3_amz_review_apparel_negative_path,error_bad_lines=False,engine='python')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.read_csv(s3_amz_review_apparel_negative_path, nrows= 8936, quoting=csv.QUOTE_NONE,error_bad_lines=False,\n",
    "#                  encoding='utf-8',engine='python')\n",
    "# df = pd.read_csv(s3_amz_review_apparel_negative_path, nrows= 8936, error_bad_lines=False,\n",
    "#                  encoding='utf-8',engine='python')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download AMAZON Review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5906333, 15)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "df = pd.read_csv(output_infer_path, \n",
    "                 delimiter='\\t', \n",
    "                 quoting=csv.QUOTE_NONE,\n",
    "                 compression='gzip')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://amazon-reviews-pds/tsv/amazon_reviews_us_Apparel_v1_00.tsv.gz is downloaded\n",
      "s3://amazon-reviews-pds/tsv/amazon_reviews_us_Beauty_v1_00.tsv.gz is downloaded\n",
      "s3://amazon-reviews-pds/tsv/amazon_reviews_us_Grocery_v1_00.tsv.gz is downloaded\n"
     ]
    }
   ],
   "source": [
    "import os, sagemaker\n",
    "\n",
    "def download_amazon_review(basic_folder = 's3://amazon-reviews-pds/tsv', prefix='', down_folder=''):\n",
    "    s3_source_path = os.path.join(basic_folder, prefix)\n",
    "#    print(s3_source_path)\n",
    "    sagemaker.s3.S3Downloader.download(s3_source_path, down_folder)\n",
    "    print(f'{s3_source_path} is downloaded')\n",
    "\n",
    "\n",
    "amazon_data = 'az_data'\n",
    "os.makedirs(amazon_data, exist_ok=True)\n",
    "\n",
    "prefix = 'amazon_reviews_us_Apparel_v1_00.tsv.gz'\n",
    "download_amazon_review(prefix = prefix, down_folder = amazon_data)\n",
    "prefix = 'amazon_reviews_us_Beauty_v1_00.tsv.gz'\n",
    "download_amazon_review(prefix = prefix, down_folder = amazon_data)\n",
    "prefix = 'amazon_reviews_us_Grocery_v1_00.tsv.gz'\n",
    "download_amazon_review(prefix = prefix, down_folder = amazon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! aws s3 ls 's3://amazon-reviews-pds/tsv' --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    " \n",
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    " \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    " \n",
    "# tokenizer : 문장에서 색인어 추출을 위해 명사,동사,알파벳,숫자 정도의 단어만 뽑아서 normalization, stemming 처리하도록 함\n",
    "def tokenizer(raw, pos=[\"Noun\",\"Alpha\",\"Verb\",\"Number\"], stopword=[]):\n",
    "    return [\n",
    "        word for word, tag in twitter.pos(\n",
    "            raw, \n",
    "            norm=True,   # normalize 그랰ㅋㅋ -> 그래ㅋㅋ\n",
    "            stem=True    # stemming 바뀌나->바뀌다\n",
    "            )\n",
    "            if len(word) > 1 and tag in pos and word not in stopword\n",
    "        ]\n",
    "# 테스트 문장\n",
    "rawdata = [\n",
    "    '남북 고위급회담 대표단 확정..남북 해빙모드 급물살',\n",
    "    '[남북 고위급 회담]장차관만 6명..판 커지는 올림픽 회담',\n",
    "    \n",
    "    '문재인 대통령과 대통령의 영부인 김정숙여사 내외의 동반 1987 관람 후 인터뷰',\n",
    "    '1987 본 문 대통령..\"그런다고 바뀌나? 함께 하면 바뀐다\"',\n",
    "    \n",
    "    '이명박 전 대통령과 전 대통령의 부인 김윤옥 여사, 그리고 전 대통령의 아들 이시형씨의 동반 검찰출석이 기대됨'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_transform, (sentence 5, feature 7)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 1 2 0 0 0 1]\n",
      " [0 1 1 0 0 0 2]\n",
      " [1 0 0 2 1 1 0]\n",
      " [1 0 0 1 0 0 0]\n",
      " [0 0 0 3 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vectorize = CountVectorizer(\n",
    "    tokenizer=tokenizer, \n",
    "    min_df=2    # 예제로 보기 좋게 1번 정도만 노출되는 단어들은 무시하기로 했다\n",
    "                # min_df = 0.01 : 문서의 1% 미만으로 나타나는 단어 무시\n",
    "                # min_df = 10 : 문서에 10개 미만으로 나타나는 단어 무시\n",
    "                # max_df = 0.80 : 문서의 80% 이상에 나타나는 단어 무시\n",
    "                # max_df = 10 : 10개 이상의 문서에 나타나는 단어 무시\n",
    ")\n",
    " \n",
    "# 문장에서 노출되는 feature(특징이 될만한 단어) 수를 합한 Document Term Matrix(이하 DTM) 을 리턴한다\n",
    "X = vectorize.fit_transform(rawdata)\n",
    " \n",
    "print(\n",
    "    'fit_transform, (sentence {}, feature {})'.format(X.shape[0], X.shape[1])\n",
    ")\n",
    "# fit_transform, (sentence 5, feature 7)\n",
    " \n",
    "print(type(X))\n",
    "# <class 'scipy.sparse.csr.csr_matrix'>\n",
    " \n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorize.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1987', '고위', '남북', '대통령', '동반', '여사', '회담']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1987</th>\n",
       "      <th>고위</th>\n",
       "      <th>남북</th>\n",
       "      <th>대통령</th>\n",
       "      <th>동반</th>\n",
       "      <th>여사</th>\n",
       "      <th>회담</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1987  고위  남북  대통령  동반  여사  회담\n",
       "0     0   1   2    0   0   0   1\n",
       "1     0   1   1    0   0   0   2\n",
       "2     1   0   0    2   1   1   0\n",
       "3     1   0   0    1   0   0   0\n",
       "4     0   0   0    3   1   1   0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=X.toarray(), columns=features)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네이버 영화 리뷰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O train.txt https://www.dropbox.com/s/374ftkec978br3d/ratings_train.txt?dl=1\n",
    "# !wget -O test.txt https://www.dropbox.com/s/977gbwh542gdy94/ratings_test.txt?dl=1\n",
    "\n",
    "train_df = pd.read_csv('train.txt', sep='\\t')\n",
    "print(\"train_df shape: \", train_df.shape)\n",
    "test_df = pd.read_csv('test.txt', sep='\\t')\n",
    "print(\"test_df shape: \", test_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
